{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Open-Loop Calibration with Radial-8"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Run the calibration graph"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","import os\n","import pickle\n","import time\n","from datetime import datetime\n","\n","import redis\n","import yaml\n","\n","DURATION = None  # seconds\n","GRAPH = 'sim_graph_ol.yaml'\n","REDIS_IP = '192.168.30.6'\n","REDIS_PORT = 6379\n","test_dir = os.getcwd()\n","\n","with open(GRAPH, 'r') as f:\n","    graph = yaml.safe_load(f)\n","\n","r = redis.Redis(host=REDIS_IP, port=REDIS_PORT)\n","\n","curs, start_streams = r.scan(0, _type='stream')\n","while curs != 0:\n","    curs, streams = r.scan(curs, _type='stream')\n","    start_streams += streams\n","\n","# get the most recent ID from each stream\n","start_id = {}\n","for stream in start_streams:\n","    replies = r.xrevrange(stream, count=1)\n","    if replies:\n","        start_id[stream] = replies[0][0]\n","\n","print(f'Starting graph from {GRAPH} as JSON')\n","r.xadd('supervisor_ipstream', {\n","    'commands': 'startGraph',\n","    'graph': json.dumps(graph)\n","})\n","\n","if DURATION:\n","    print(f'Waiting {DURATION} seconds')\n","    time.sleep(DURATION)\n","else:\n","    input('Hit ENTER to stop graph...')\n","\n","# Stop the graph\n","print('Stopping graph')\n","r.xadd('supervisor_ipstream', {'commands': 'stopGraph'})\n","\n","curs, stop_streams = r.scan(0, _type='stream')\n","while curs != 0:\n","    curs, streams = r.scan(curs, _type='stream')\n","    stop_streams += streams\n","\n","new_streams = [\n","    stream for stream in stop_streams if stream not in start_streams\n","]\n","\n","for stream in new_streams:\n","    start_id[stream] = 0\n","\n","# Save streams\n","all_data = {}\n","for stream in stop_streams:\n","    all_data[stream] = r.xrange(stream, min=start_id[stream])\n","\n","date_str = datetime.now().strftime(r'%y%m%dT%H%M')\n","graph_name = os.path.splitext(os.path.basename(GRAPH))[0]\n","data_dir = os.path.join(test_dir, 'data')\n","os.makedirs(data_dir, exist_ok=True)\n","save_path = os.path.join(data_dir, f'{date_str}_{graph_name}.pkl')\n","with open(save_path, 'wb') as f:\n","    pickle.dump(all_data, f)\n","print(f'Saved streams: {sorted(list(all_data.keys()))}')\n","\n","# Remove saved data from Redis\n","# delete any streams created while the graph was running\n","i = 0\n","if new_streams:\n","    while max([r.xlen(stream) for stream in new_streams]):\n","        for stream in new_streams:\n","            r.delete(stream)\n","        i += 1\n","r.memory_purge()\n","print(f'Deleted streams: {new_streams}')"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Analyze the block"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","import os\n","import pickle\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import yaml\n","from brand.timing import timespecs_to_timestamps, timevals_to_timestamps\n","from scipy.signal import butter, sosfiltfilt\n","from sklearn.linear_model import RidgeCV\n","from sklearn.model_selection import train_test_split\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# constants\n","test_dir = os.getcwd()\n","data_dir = os.path.join(test_dir, 'data')\n","fig_dir = os.path.join(test_dir, 'figures')\n","data_file = save_path\n","\n","# setup\n","os.makedirs(fig_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(os.path.join(data_dir, data_file), 'rb') as f:\n","    graph_data = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sorted(graph_data.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load graph parameters\n","graphs = [\n","    json.loads(entry[b'graph']) for _, entry in graph_data[b'booter']\n","    if b'graph' in entry\n","]\n","graph = graphs[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load info about the structure of each stream\n","with open('stream_spec.yaml', 'r') as f:\n","    stream_spec = yaml.safe_load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load and parse stream data\n","streams = [\n","    b'targetData', b'cursorData', b'mouse_vel', b'binned_spikes',\n","    b'firing_rates', b'nsp_neural_1', b'nsp_neural_2', b'control'\n","]\n","decoded_streams = {}\n","for stream in streams:\n","    print(f'Processing {stream.decode()} stream')\n","    stream_data = graph_data[stream]\n","    out = [None] * len(stream_data)\n","    spec = stream_spec[stream.decode()]\n","    for i, (entry_id, entry_data) in tqdm(enumerate(stream_data)):\n","        entry_dec = {}\n","        for key, val in entry_data.items():\n","            if key.decode() in spec:\n","                dtype = spec[key.decode()]\n","                if dtype == 'str':\n","                    entry_dec[key.decode()] = val.decode()\n","                elif dtype == 'sync':\n","                    entry_dec[key.decode()] = json.loads(val)['nsp_idx']\n","                elif dtype == 'timeval':\n","                    entry_dec[key.decode()] = timevals_to_timestamps(val)\n","                elif dtype == 'timespec':\n","                    entry_dec[key.decode()] = timespecs_to_timestamps(val)\n","                else:\n","                    dat = np.frombuffer(val, dtype=dtype)\n","                    entry_dec[key.decode()] = dat[0] if dat.size == 1 else dat\n","        out[i] = entry_dec\n","    decoded_streams[stream.decode()] = out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load data at the binned spikes sample rate\n","# FSM\n","cd_df = pd.DataFrame(decoded_streams['cursorData'])\n","cd_df.set_index('sync', drop=False, inplace=True)\n","cd_df.columns = [col + '_cd' for col in cd_df.columns]\n","\n","td_df = pd.DataFrame(decoded_streams['targetData'])\n","td_df.set_index('sync', drop=False, inplace=True)\n","td_df['angle'] = np.degrees(np.arctan2(td_df['Y'], td_df['X']))\n","td_df.columns = [col + '_td' for col in td_df.columns]\n","\n","# binning\n","bs_df = pd.DataFrame(decoded_streams['binned_spikes'])\n","bs_df.set_index('sync', drop=False, inplace=True)\n","bs_df.columns = [col + '_bs' for col in bs_df.columns]\n","\n","# autocue\n","ac_df = pd.DataFrame(decoded_streams['control'])\n","ac_df.set_index('sync', drop=False, inplace=True)\n","ac_df.columns = [col + '_ac' for col in ac_df.columns]\n","\n","# join the dataframes\n","bin_df = cd_df.join(td_df).join(bs_df).join(ac_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check the NSP data\n","tslice = slice(0, 4000)  # range of samples to inspect (units: sample index)\n","for nsp_id in [1, 2]:\n","    n_channels = graph['nodes'][f'nsp_in_{nsp_id}']['parameters'][\n","        'chan_per_stream'][0]\n","    samp_per_stream = (\n","        graph['nodes'][f'nsp_in_{nsp_id}']['parameters']['samp_per_stream'][0])\n","    nsp_df = pd.DataFrame(decoded_streams[f'nsp_neural_{nsp_id}'])\n","    nsp_data_1 = np.hstack(nsp_df['samples'].apply(\n","        np.reshape, newshape=(n_channels, samp_per_stream))).T\n","\n","    # Truncate data to the requested time slice\n","    nsp_data_1 = nsp_data_1[tslice, :]\n","\n","    # Load filter parameters\n","    tc_params = graph['nodes'][f'thresh_cross_{nsp_id}']['parameters']\n","\n","    but_low = tc_params['butter_lowercut']\n","    but_high = tc_params['butter_uppercut']\n","    but_order = tc_params['butter_order']\n","\n","    if but_low and but_high:\n","        filt_type = 'bandpass'\n","        Wn = [but_low, but_high]\n","    elif but_high:\n","        filt_type = 'lowpass'\n","        Wn = but_high\n","    elif but_low:\n","        filt_type = 'highpass'\n","        Wn = but_low\n","    else:\n","        raise ValueError(\"Must specify 'butter_lowercut' or 'butter_uppercut'\")\n","\n","    fs = tc_params['input_stream']['samp_freq']\n","    sos = butter(but_order, Wn, btype=filt_type, output='sos', fs=fs)\n","\n","    if tc_params['enable_CAR']:\n","        nsp_raw = nsp_data_1 - nsp_data_1.mean(0, keepdims=True)\n","    else:\n","        nsp_raw = nsp_data_1\n","\n","    nsp_data_1_filt = sosfiltfilt(sos, nsp_raw, axis=0)\n","\n","    # Load spike thresholds\n","    threshold_stream = f'thresh_cross_{nsp_id}_thresholds'\n","    thresholds = np.frombuffer(\n","        graph_data[threshold_stream.encode()][-1][1][b'thresholds'],\n","        dtype=stream_spec[threshold_stream]['thresholds'])\n","\n","    # Check filtering\n","    plt_channels = 9  # number of channels to plot\n","    tslice = slice(0, 4000)\n","    ncols = np.ceil(np.sqrt(plt_channels)).astype(int)\n","    nrows = np.ceil(plt_channels / ncols).astype(int)\n","    fig, axes = plt.subplots(ncols=ncols,\n","                             nrows=nrows,\n","                             figsize=(ncols * 3, nrows * 2),\n","                             sharey=False)\n","    for iax in range(plt_channels):\n","        ax = axes.flat[iax]\n","        nsp_raw = nsp_data_1[tslice, iax]\n","        ax.plot(nsp_raw, alpha=0.5, label='original')\n","        ax.plot(nsp_data_1_filt[tslice, iax], alpha=0.5, label='filtered')\n","        ax.plot(thresholds[iax] * np.ones(nsp_raw.shape[0]), label='threshold')\n","        ax.set_title(f'Ch. {iax}')\n","\n","    for iax in range(plt_channels, len(axes.flat)):\n","        axes.flat[iax].set_axis_off()\n","\n","    axes.flat[0].legend()\n","    plt.tight_layout()\n","    plt.subplots_adjust(top=0.95)\n","    plt.suptitle(f'NSP {nsp_id}')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Train a decoder"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train a decoder\n","SEQ_LEN = 10  # sequence length for the Wiener filter\n","\n","\n","def get_lagged_features(data, n_history: int = 4):\n","    \"\"\"\n","    Lag the data along the time axis. Stack the lagged versions of the data\n","    along the feature axis.\n","\n","    Parameters\n","    ----------\n","    data : array of shape (n_samples, n_features)\n","        Data to be lagged\n","    n_history : int, optional\n","        Number of bins of history to include in the lagged data, by default 4\n","\n","    Returns\n","    -------\n","    lagged_features : array of shape (n_samples, n_history * n_features)\n","        Lagged version of the original data\n","    \"\"\"\n","    assert n_history >= 0, 'n_history must be greater than or equal to 0'\n","    seq_len = n_history + 1\n","    lags = [None] * seq_len\n","    for i in range(seq_len):\n","        lags[i] = np.zeros_like(data)\n","        lags[i][i:, :] = data[:-i, :] if i > 0 else data\n","    lagged_features = np.hstack(lags)\n","    return lagged_features\n","\n","\n","neural_stream = 'binned_spikes'\n","kin_stream = 'control'\n","\n","neural_data = np.vstack(bin_df['samples_bs'])\n","neural_data = get_lagged_features(neural_data, n_history=SEQ_LEN - 1)\n","kin_data = np.vstack(bin_df['samples_ac'])[:, :2]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(neural_data,\n","                                                    kin_data,\n","                                                    test_size=0.25,\n","                                                    shuffle=False)\n","# Fit the Ridge regression model\n","# Use k-fold cross-validation to select the weight of the L2 penalty\n","mdl = RidgeCV(alphas=(0.1, 1.0, 10.0), cv=3)\n","mdl.fit(X_train, y_train)\n","y_test_pred = mdl.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mdl.score(X_test, y_test_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the trained model\n","file_desc = data_file.split('_')[0]\n","\n","model_dir = 'models'\n","os.makedirs(model_dir, exist_ok=True)\n","model_path = os.path.join(model_dir, f'{file_desc}_wf_seq_len_{SEQ_LEN}.pkl')\n","\n","with open(model_path, 'wb') as f:\n","    pickle.dump(mdl, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Update the config of the closed-loop graph to load the saved model\n","cl_graph_path = 'sim_graph_cl.yaml'\n","with open(cl_graph_path, 'rb') as f:\n","    cl_graph = yaml.safe_load(f)\n","\n","node_names = [node['name'] for node in cl_graph['nodes']]\n","wf_idx = node_names.index('wiener_filter')\n","\n","cl_graph['nodes'][wf_idx]['parameters']['model_path'] = os.path.abspath(\n","    model_path)\n","cl_graph['nodes'][wf_idx]['parameters']['seq_len'] = SEQ_LEN\n","\n","# Save the edited config\n","cl_graph_gen_path = list(os.path.splitext(cl_graph_path))\n","cl_graph_gen_path.insert(-1, '_gen')\n","cl_graph_gen_path = ''.join(cl_graph_gen_path)\n","\n","with open(cl_graph_gen_path, 'w') as f:\n","    yaml.dump(cl_graph, f)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10 (default, Mar 13 2023, 10:26:41) \n[GCC 9.4.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":2}
